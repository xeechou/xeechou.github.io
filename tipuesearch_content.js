var tipuesearch = {"pages":[{"title":"About Me","text":"Hi, I am Sichem, a software developer working in the computer graphics industry à Montréal, Québec. Earlier I graduated from a master degree under computer science, targeting the research area of computer vision and computer graphics. I am also an active linux hacker, currently working on a wayland project . Aside from programming life, I enjoy sports like basketball, ski, etc, and traveling if I coould find the time. You can find my résumé here .","tags":"pages","url":"/pages/about-me.html","loc":"/pages/about-me.html"},{"title":"Back with Org Mode","text":"If there is one thing I'd like to commit it must be the my time mangement improvements. For that purpose, Emacs Org-mode is the ultimate tool for programmers. I can keep my notes nicely and I can track my tasks/errands all together. But all my notes are stored in my linux box, I lose access to them once I am away. From that reason, I was always on-and-off with using org-mode due to the portability. We are not in the age doing everything with desktop anymore, more often I need to track my TODOs from cellphone as well. A long time ago my tooling of choice was git . It seems the perfect tool, optimised for text tracking, and potentially we can use it for text merging. Mannually typing git add * , git commit all the time is annoying, luckly we can do that with a script. # sync everything git pull ; git add * ; git commit -m \" $( date ) \" git push While this is doable on the linux box, and it was my perferrable method when I was synchronizing notes among my desktops. On cellphones, however, it is not the most convenient way. I had to open pocket-git , pull, resolve conflicts, then on Orgzly , sync with git folder again. everytime I commit would have to pull immediately on the cellphone to avoid conflicts otherwise there were always conflicts for some reason. Syncthing was the another trial. It is decentralized, you can use it to synchronize any file from different computers so long as we are online, eventially they would be synchronized. Its decentralized nature is both the good and devil here, I don't have to connect/trust to any servers. My files are with me and only with me. But then I have to keep my devices online for synchronize. It is obviously not the case. Then I turned to Webdav, at the moment, the best solution among all, given that I can find a Webdav server close-enough. Nextcloud providers are the best choices, sadly all of them are far away from me. Setting up my own nextcloud on a nextcloud is an overkill and much more expensive than necessary. Luckly at where I am, there is a free option, fast enough (only 10ms from me) but a little confusing to setup. The Webdav server used requires the following options to turn on for mounting davfs: #sample from davfs2.conf if_match_bug 1 #otherwise cannot write via davfs ignore_dav_header 1 #otherwise davfs failing connect to the server At the first I could not mount the davfs. Based on what people posted online, this specific webdav server generates wrong capabilities and requires ignore_dav_header set for mounting. Then just when I was happy about finding a fast webdav server, I found out I cannot write to the davfs mount point. It bugged a whole day, disabling the cache, disabling the delay, nothing works. I was about to give up, falling back using cadaver util. I found out folks on launchpad were experiencing the same problem. Given nothing else works, I just gave it a try and it was like magic. This serves as a warning, DO NOT expect the private web service you are using works as expected. This is probably the last block on my way. Now I can sync the note by one guesture on the phone. For summng up, on desktop: mount the webdav server using davfs. Work, coding, capturing on emacs as usual. umounting once away from keyboard. on cellphone: 1. open the APP, drag to sync, 2. performing org operations if necessary. 3. tap to sync again. Directly editing remote files maybe laggy in emacs if the server is far away. For that reason, in the future, I may write a lisp function for pulling and pushing on emacs as well. Orglzy is my current mobile app of choice. It is not the most pretty one but supports editing, if not require editing, Orgro supports more syntax highlighting. Organice is another Webapp I can use but unfortunately not compatible with my current webdav server.","tags":"misc","url":"/back-with-org-mode.html","loc":"/back-with-org-mode.html"},{"title":"Xwayland Clipboard","text":"I hate every single line of xwayland code I wrote, it is ugly, long and hard to maintain. Now I know very well why there is wayland in the first place, sadly a complete wayland compositor has to bring a piece of crap of X with it. I just finished the selection handling in xwayland by mimicing weston code. After finally understand that 1000 lines of confusing code, I just found out I can't do any better. Handling X11 selection revolves around 4 events: XSelectionNotify , XSelectionRequest , XPropertyNotify and XfixesSelectionNotify . Their names are just as confusing as how they work. It gets more complicated when INCR protocol is involved. Our job in compositor, for xwayland, is a proxy. That means getting wl_data_source to X data_offer , and writing to wl_data_offer from a X data_source when there is no data_offer and data_source in X at all. In wayland, the protocol utilises PIPE for implementing clipboard. In X, it is implemented by writing to a property of a X window. If you have no idea about the properties in X. It is like a key value hash map, every window has a such hash map. Clients communicates by reading/writing properties. Here is catch, the developers ought to adher the specificiation, ICCCM for correctly implementing their clients or XWM. If anything goes wrong... Back to our topic, how do we work with those 4 events? Let's walk through the story. When a client press \"ctrl-c\", it declares the owner of CLIPBOARD, which is a special atom in the X. Nothing happens at the moment, until another client, the requestor press \"ctrl-v\", it then search for the owner of the CLIPBOARD and ask for data, it begins our first event XSelectionRequest , basically it says, \"hey, the owner, I would like you to convert what you have in the CLIPBOARD to the target I asked and write to my property that I tell you\". Then the owner can use a function XConvertSelection for that purpose. When it finishes, the owner send XSelectionNotify event to the requestor . The requestor indicates the end of the transaction by deletes the property. We didn't mention the other two events. It has to do with our xwayland usage. I tried to summary the transaction as brief as possible, but we also omitted a lot of details. There are other playes, the wm(part of your wayland compositor), the CLIPBOARD_MANAGER and the role of xserver(xwayland in this case) in the story. Let me start by asking a few questions: How does the XWM know about the onwer of the CLIPBOARD? How does a wl_data_source becomes the owner of the CLIPBOARD? How do we copy the property to wl_data_offer or vice versa? When a owner of the CLIPBOARD annouces itself in Xwayland, the XWM need to make a wl_data_source out of it. This is done through the XfixesSelectionNotify event. In the event, we, the XWM, would ask for all the targets (or mimes in wayland terms) that owner has(not the data itself). With that, we have enough to create a proxy wl_date_source , if there is no wl_client ask for data, we do nothing more, otherwise, the XWM represents the wl_client asking data from X. On the other end, if a wl_data_source becomes available, the XWM gets notified and simply declares the owner of CLIPBOARD, if we are to process any XSelectionRequests , a PIPE need to create inside the XWM for reading from wl_data_source and writes the data to some X properties. In the end, the selection data flows like this. The ugly part is when INCR protocol comes in. INCR is for the case when you could not read/write X properties in one shot. Basically the XRequestor need to delete the property selection owner wrote to and wait for it to write again. For us xwayland case, we are in the akward situation for waiting on both X client deleting property and PIPE fd becomes available for IO .","tags":"misc","url":"/xwayland-clipboard.html","loc":"/xwayland-clipboard.html"},{"title":"Jan 2021 Status Update","text":"Folks, the Magical year of 2020 is behind us, and it wasn't a happy new chapter waiting us. Man, I still couldn't believe, once per 100 years, why it was us who have to face the coronavirus. Now I take a good look of my 2020, I went from trying to submit a xcbcommon handling patch to weston to writing a full wayland compositor from scratch. It was not my plan!!! Taiwins should have being a rather simple application on top of the libweston, the so called \"reference compositor \", but libweston cannot change keyboard layout once launching, nor it can support cloning output, there are new bugs when you digging deep. The worst is that it breaks the library into different shared modules, and changes lib location every release. It was like it really does not want to link to it. The codebase was old and long, it was difficult to make a change and really difficult to get libweston taking a patch. Then there is the wlroots. If it was released half year earlier Taiwins would probably sit on top of that. But, screw it. I am doing it myself. It really took all my time, my free time and some time I should probably do other things. It was this magical year that I can do it, starting from writing abstraction of of display and inputs, to plug unplug my TV at home for testing my drm code. I really never expected the amount of knowledge expected for writing the compositor. Now I have a whole set of knowledge which is not useful elsewhere. Think about I could use my time writing some other things, like a feature Vulkan renderer, or learning financing knowlege I am clearly lack of. The Biggest struggle is obvious that why I am doing this not getting paid? Was it really worth it? Sometimes I feel like a person forgotten in the past, not really living in this world. The time of desktop compositor is over. Will taiwins really going to be useful to others? When everyone converges to Gnome and KDE? Oh wait, there is the sway, seems it pretty much takes over the rest of the market. I do not have the answer. The worst case it would be me as the only user. It is really sad when I start to think about this. Alright there, stop to get too sentimental, Taiwins should have a pending alpha release at end of this month, or maybe a little bit later next month, I mean you have to look to the future right?","tags":"misc","url":"/jan-2021-status-update.html","loc":"/jan-2021-status-update.html"},{"title":"Working with libdrm, buffer allocation","text":"In the first blog of libdrm , we went through the repainting loop using the libdrm. Now we move on to the topic of framebuffers. A framebuffer is a piece of memory (could be on main memory or on GPU) for repainting every frame. It is like a canvas, represents what you would finally see on screen. Framebuffer is already a familiar concept to rendering programmers, in OpenGL , We have GL_FRAMEBUFFER_0 for presenting the surface, additional framebuffers for roles like G-Buffer and post-processing. In GPU programming, framebuffer is also a broader concept, \"abstraction of a collection of attachments\", the underlining attachments corresponds to actual pieces of memory we talk about here. Dumb buffer Back to our topic, we have two options for allocating framebuffers for libdrm. The first is through dumb buffer, a slow, linear, in main memory buffer. Creating that, we have drmioctl(fd, DRM_IOCTL_MODE_CREATE_DUMB, &req) . the req argument here specifies parameters like the width, height, pitch, bit-per-pixel. For retriving a framebuffer handle, libdrm provides function drmModeAddFB(fd, width, height, depth, ...) . Then we map the buffer though good old mmap function and do all kinds of magic on it, very 1990s ish. Finally , having the fb handle, we would use either drmModeSetCrtc or drmModePageFlip to present that buffer on screen. GBM buffers The second method is through device GEM buffers, they are usually the GPU memory, supports compression, modifiers and of course, GPU commands. In the open source world, Mesa project provides a generic libgbm for vendor agnostic allocations, we get framebuffer handle from libgbm through the gbm_bo object. libgbm is rather flexible, for getting the gbm_bo s. We can either go with the traditional method, allocating gbm_surface and getting the buffers through gbm_surface_lock_front_buffer , or we can mannually creating them with gbm_bo_create[_with_modifiers] . The first approach reflects to the OpenGL rendering api, A gbm_surface maps to a EGLSurface , gbm_surface_lock_front_buffer can only be called after eglSwapBuffers . The disadvantage is obvious as well, it is tied to the OpenGL pipeline and we have no control of how many gbm_bo s to allocate. As it is tied to OpenGL pipeline. In Vulkan, there is no mapping of VKSurfaceKHR to GBM, for that, we will go through the details later. EGLStream EGLStream is the Nvidia's proprietary approach on supporting libdrm, as apparently they don't use GEM buffers, they came up with this EGLStream stack for working with their driver blob. It is not simply a buffer allocation API as it also introduces EGLDeviceEXT , EGLOutputLayerEXT . Using EGLStream , there are fixed steps to follow: discovering an EGLDeviceEXT which supports drm flags, getting a EGLDisplay from opened device using EGL_DRM_MASTER_FD_EXT . creating a EGLStreamKHR , getting an output layer(plane) using eglGetOutputLayersEXT . attach plane as consumer using eglStreamConsumerOutputEXT . creating EGLSurface with the eglCreateStreamProducerSurfaceKHR as the producer. driving pageflipping using eglStreamConsumerAcquireAttribNV . Vulkan workflow Vulkan has its own path in supporing direct compositing. On Vulkan side, there is an extension called VK_DISPLAY_KHR , very much like the EGLStream extension, trying to do everying inside vulkan. You can create a VkSurface with VkDisplayKHR through vkCreateDisplayPlaneSurfaceKHR , once you have the surface, everything else would look like a typical Vulkan application as well. The downsides is that you also lose access of direct modesetting control like drmModeSetCrtc or drmPageFlip , and there is no plane assignment. If you were previously using libdrm, it also means you need to throw-the-code-out-of-window. It is a viable approach if you decide to go with Vulkan and only Vulkan, as some people prefer this approach but not every one. For people wanting an alternative way(Vulkan + KMS WSI), The journal of the discussion is here . The problem is that unlike OpenGL, Vulkan is an \"explicit\" API, As tomek-brcm wrote: Currently GLES/EGL implemntation of Wayland compositor and clients is based on a silent assumption that driver has an implicit cross-process synchronisation. This is not GLES or EGL requirement, just an unwrittenWayland assumption. Vulkan is all about explicit synchronisation that isresponsibility of an application but it doesn't provide any cross-processsynchronisation primitives. This is, to my untrained eye, a blocker issue. We get the client buffer on the wl_surface.commit but there is no guarantee what-so-ever about client has finished drawing. If a wayland server has to wait on a fence every time a client commits a buffer. It is probably not very usable. Luckly the issue was resolved through the VK_KHR_external_*_extension when Vulkan lands on 1.1. They enable the implementation of all the Vulkan WSI mechanisms/extensions on top of core Vulkan + the OS-specific parts of external objects .","tags":"misc","url":"/working-with-libdrm-buffer-allocation.html","loc":"/working-with-libdrm-buffer-allocation.html"},{"title":"Working with libdrm, repainting timeline","text":"As the taiwins project finished with X11/Wayland backend rigging. Now I am fully on the libdrm backend development, dealing with hardwares directly. This backend requires 200 percent my energy to tame the complexity. Like other backends, libdrm also needs to provide two resources, input and output . Backends like nested wayland backend or X11 backend, the output device are the windows created by ourselves (or by the user). Meaning we can create it or destroy it as we please. As for libdrm, those are provided by the OS. An output in libdrm's terminology, is called connector. We query and change the connectors state by drmModeGet* functions. Then later set the hardware states by drmModeSet* or the new atomic API. It may sound simple but actually, for displaying images on the screen using the libdrm mechanism, it takes quite a few components, working together, to achieve the goal. We uses drmMode* routines for kernel-modesetting ; libgbm for framebuffer allocation; OpenGL or Vulkan for rendering. If done right, you would get tear-free images on your screen constant refresh rate. But should anything goes wrong, you probably end up with a black screen. I plan to create this series of blog of libdrm while I work on the taiwins drm-backend. Hopefully by the end of the month, I would have all the pieces together. In this Part I , we will look at the drm repainting loop. DRM repaint timeline Knowing vaguely the concepts is not sufficient to come up with a working setup, we want to know WHEN to do WHAT in order to get things right. For a real world reference, here I steal an image 1 of the weston repaint timeline. We can see from the image, during every frame, the compositor does its rendering and tries to hit the vblank for displaying the result. the vblank is a timestamp here, it is used to be a period where the CRT monitors move electron gun from the bottom to the top. It is also the period for setting gamma and swapping image buffer. For now we just need to know that at vblank , there is a buffer swapping, also called page-flip . If a client has double-buffering, it would want to Before vblank , draw on the back-buffer. After vblank , the back-buffer was used and it can draw on the new buffer. Translating into code(here we use drmWaitBlank ): do { drmVBlank vbl = { . request . type = DRM_VBLANK_RELATIVE , . request . sequence = 1 , } ; drmWaitVBlank ( fd , & vbl ); // fd is the opened GPU // vblank returns , with the timestamp struct timespec spec = { . tv_sec = vbl . reply . tval_sec , . tv_nsec = vbl . reply . tval_usec * 1000 , } ; // repaint with new frame , return with new frame buffer , int fb_id = repaint (); // tell drm to use the new fb_id on present drmModeSetCrtc ( fd , crtc_id , fb_id , 0 , 0 , conn_id , mode ); send_present_event ( & spec ); } while ( ! quit ); In the snippet above, we wait for a vblank, then render and setCrtc in one shot. The obvious problem here is we have control of the duration of the rendering, it could exceed the vblank time window. Also there is a time we are waiting for nothing. This is not exactly what we want, luckly libdrm has a drmModePageFlip we can use. static void handle_page_flip ( int fd , unsigned int frame , unsigned int sec , unsigned int usec , void * data ) { int fb_id = repaint (); struct timespec spec = { . tv_sec = sec , . tv_nsec = usec * 1000 , } ; drmModePageFlip ( fd , crtc_id , fb_id , DRM_MODE_PAGE_FLIP_EVENT , data ); send_present_event ( & spec ); } while ( ! quit ) { drmEventContext ev = { . version = 2 , . page_flip_handler = handle_page_flip , } ; poll ( fd ); // the opend gpu . drmHandleEvent ( fd , & ev ); } This is a better example, which I modified from here 2 . This time we drawing on demand, using the non blocking drmModePageFlip to set buffer on next vblank. Note that the flag DRM_MODE_PAGE_FLIP_EVENT request the new page flip events for drawing, without it, we would not get notified. The potential problem is we may miss the next vblank if rendering takes too long (Triple-buffering with multi-threads could help). Weston repaint scheduling from Pekka Paalanen. ↩ modeset-vsync from David Rheinsberg. ↩ drm_doc by ascent12. ↩","tags":"misc","url":"/working-with-libdrm-repainting-timeline.html","loc":"/working-with-libdrm-repainting-timeline.html"},{"title":"August 2020 Status Update","text":"It is a sunny day, somehow I got up early this morning. The cool, chilling breath of air reminded me we are at the tail of the summer. It felt so strange, this summer was like it was never here. Hiking, go swimming, taking a trips and night outs. all those experiences for a normal summer I would do, they are all absent, hope I don't regret too much for crunching code at home all these months. It was a tough year actually, I was determined to push Taiwins to a usable state, it has been stressful. Every time I look at the pile of the TODO list, a sentiment of endless development and desperate hit me hard like a heart attack. I had to work a lot to stay healthy. Here I am taking a clear picture of my memory from April. In the begining it was only getting a window to show up, and getting wl_seat to work, then I needed to draw surfaces, that is implementing wl_surface , binding textures from wl_buffers and paint them on the window, I had to writing dummy clients which normally shouldn't work because wl_surface requires a \"role\" (like xdg_toplevel ). Rigging up the surface transformation code so it rotates and scale correctly. Boy, those transformations, I realised my understanding of Inverse Matrix was not correct at all. Subsurfaces was developped along the way, I didn't even know the usage at the begining, until I read this blog-post . Back then when I was writing wayland client library, I needed not care about the protocol I don't understand, but now I have no choice now I guess. Then here we go, the xdg_surface . The one that gives you all the desktop applications, turns you surfaces into xdg_toplevel and xdg_popup . The difficulty was that I want to implement the libweston-desktop style interface so I can interface with wl_shell_surface as well. xdg_surface was way more complicated, this is the point where I ran into the wall of wl_object dependency , where I found clients offer destruct wl_surface , xdg_surface , xdg_toplevel , xdg_popup in the arbitrary orders. Boy I hope I implement the dependency correctly, now I do not run into dangling pointers (or I think)? Plenty of protocols like wp-viewporter and wp-presentation were implemented along the way, those were \"easier\" one, did not give me many hiccups. Now I am on the way implementing an efficient renderer. Interestingly even with only a basic renderer, the renderer is still performing at sub-millisecond per frame. Now compared to June or May, I felt much better, back then it felt like mission impossible, I tried to focus on the moment, try not think about how it can accomplish like KWin. But who I am kidding, With myself, certainly NO, only if I can get more people engaged in the project, the one thing I am not good at.","tags":"misc","url":"/august-2020-status-update.html","loc":"/august-2020-status-update.html"},{"title":"July 2020 Status Update","text":"July, the hottest month in Montreal every year. The heat wave rushes to you when you walk outside makes you wondering if you are in some tropical island, it sure doesn't look like living at 45 latitde northen hemisphere. Last month was a rapid leap towards wayland objects implementations and now I just hitted the wall of xdg-shell protocol. Today I'd like to talk about what it is like to implement a wayland protocol. there are really two parts of the story, the client and the compositor. Here I am only elaborate on the compositor side, protocols are usually (and it should be designed) easier to use on client side, thus the compositors takes most the workload, which is fine, there are way more clients than compositors. A wayland protocol is a wl_global , so the first step is mostly the same. wl_global_create ( display , interface , version , data , binding_functions ). void binding_functions ( struct wl_client * client , void * data , uint32_t version , uint32_t id ) { ... } From this point on, you created a wl_resource object and work with its interface, which is a list of function calls. Then, you would probably run into two scenarios. You found out the compositor already has most of the required functions, so implementing a protocol is a nature externsion. The compositor does not have the functions so you have to implement a protocol interface and complete that interface with the compositor later. As for my case with taiwins. Most of the time I ran into scenario 2. Simple protocols like wp_viewporter is fine, they are usually works like getter and setter functions. That you know you probably would not make many mistakes. Depends on the number of client requests, the workload could be one to a few hours. The complex protocols, like xdg_shell , on the other hand, are usually compound. It may contains a few sub-protocols and interacts with each other. This is when things gets a bit tricky. You can implement the interface as far as you can go but you are never sure how correct the implementation is. For instance with xdg_shell.xdg_surface . it can turn into either a xdg_popup or xdg_toplevel , Then xdg_toplevel and generate a xdg_popup , xdg_popup can later start a grab and you can have maintain the popup states. The implemenation is easily vunlerable for bugs. One tricky thing particularly is the object dependencies in wl_resource . The destruction of the objects does not follow the creation. Be careful you can easily end-up with a dangling pointer. The example here is xdg_surface and wl_surface , while you need a wl_surface to create a xdg_surface but the wl_surface could destruct before the xdg_surface . I've bitten a few times already and it is surely an annoying thing. For xdg_shell , it took me a day or two to roughly fill up the request callbacks, it was like walking on thin air. Then another day to actually implement the compositor actual functions for testing. The day after I could finally grab a few real life applications for debugging, and it crashed... * This is the complex protocols, the stack of implementation is really long before actual tests can happen. I sincerely hope next time I can complain less.","tags":"misc","url":"/july-2020-status-update.html","loc":"/july-2020-status-update.html"},{"title":"June 2020 Status Update","text":"Montréal is hotter than ever in this month, under the broiling sun, it was an exhausting quarantine. The first month I remeber I was feeling fortunate I got this aleatory chance for advancement of taiwins in this global pandemic, right now all I felt was the painful experience converting all the Taiwins server code away from libweston. Implementating of server side wayland objects was indeed unpleasant experience. Without libweston, I have to implement every little thing and get it right, even the basics 2D homogenous matrix operations for coordinates transformations. Just a days I ago I discovered my mistakenly used rotations in the \"Y-up\" coordinate system (like OpenGL) in the \"Y-down\" coordinate sytem (like Wayland protocol). The good news was that I did not give up, it seemed I overcame one difficulties after another, now at least I can paint my taiwins shell objects using a primitive renderer. In the end, hopefully I will end-up with a working and cleaner compositor library than libweston . This is certainly an encouraging news as I used to think a compositor library was not possible to finish with only two hands. Aside from crunching for taiwins, there wasn't much going on. Are we going out of this pandemic or expecting a second wave? Will we have a vaccine? I heard the virus has mutated, would the vaccine still be valid when it came out? This pandemic is certainly unprecendented for this generation, people are graduating online. global airlines are pratically dead. The city is half live and half dead. Just when you thought things are getting better then recently the massive BLM protests took on. So nope, we are going violent and chaotic all the way. Sadly I may leave Montreal for good this year. It felt like a relationship turned south, When I grab my luggage and get on the airplane a few month later, it would not be much a pleasant goodbye. Maybe I am too dramatic here, this is merely the aftermath of this exhausting pendamic, I am tired inside out. At least the stocking market is going rocket high right? Well, anyway, I guess there are many things we have no control over, I only hope we will coming out stronger.","tags":"misc","url":"/june-2020-status-update.html","loc":"/june-2020-status-update.html"},{"title":"Libweston vs Wlroots","text":"You need a wayland compositor library to create a wayland server. Wayland is a protocol, itself, cannot do much, long time ago I used to think the server-side wl_resource represents some structure you can operate on, well it turned out all you do with it is handling messages between client and server. All the logic you need to implement yourself. There were a few attempts to wayland compositing library created. The first sucessful one is wlc . I think its existence is that weston (the twin of the wayland project, now maintained by Pekka Paalanen). was a monolith application for a long time. Weston looks pretty and quite fast and beautiful but not very useful to anyone else. The developers maybe realized that they want to get more people to use weston, so they managed to strip libweston out of weston itself. It was a good attempt, although I doubt their true intention was to creating a reusable compositing library. Nonetheless, to some extent, it helped the birth of the third library by Drew Devault, the wlroots . Over this month, I am trying to migrate from libweston. Over these years, I slowly discovered that libweston has its hidden flaws that need to be patched, I tried a few times to send Merge requests, now I lost my faith and gave up on patching libweston. Well, it is hard to argue on a project you do not maintain. The feature you thought would be important may not be the same for others. The target platform, is the wlroots. This time however, I learned from my previous mistake, a open source project is like a person, you cannot expect it to be something it has not been yet . This time, I merely use wlroots as a thin layer for hardware abstraction(if there is any good alternative I would ditch wlroots for sure). Libliftoff looks like a good candidate, depends on if it gets mature. Maybe in the end I would have to write one myself. A brief view Anyway, the purpose today is comparing libweston and wlroots . Though Drew Devault claimed wlroots is a much better choice, I see them similar in many ways, in implementing many protocols. wayland types libweston wlroots wl_output weston_output wlr_output head weston_head wlr_output wl_surface weston_surface wlr_surface view weston_view wlr_surface_state wl_seat weston_seat wlr_seat compositor internal wlr_compositor data_device internal wlr_data_device input modifier grab system grab system rendering internal wlr_output.frame Though many types in libweston you can find correspondence in wlroots , some of the implementation are also similar, eg, data-device, but you can also find the significant difference. In libweston , you have the weston_compositor as a middle layer, it does all the heavily lifting, as a user, you would add decorations on top of it, for example, you could add some callbacks on output creation. On the land of wlroots , you will be heavily rely on wl_signal . The library notifies you when to do something , but to do what and how to do is totally up-to-you. As good as it sounds, wlroots also has its limitations, by design, you are almost forced to use none-or-all of its types. wlr_compositor depends on wlr_renderer , wlr_data_device depends on wlr_seat , the list goes on. Some of the dependencies to me were not necessary, but again, if I propose to change it, highly likely would be rejected. Also, by its design, wlroots implements similar protocols in different types, eg, xdg_shell , xdg_shell_v6 and wl_shell . You would have to deal with the interface one-by-one, In libweston, they are nicely implemented together and it exposed libweston_desktop API to you. Now let us have a run using the two library, to get a more concrete idea. A quick run into libweston. You start by creating wl_display and creating weston_compositor object first(it self already does quite a few things, implement many protocols). Before waking the compositor, you would want to setting some listeners and signals on seat , output creations. It is also the good time for adding the callbacks for libweston_desktop . Libweston comes with a few backend, like drm_backend , x11_backend , fbdev_backend , those are the backends implements weston_output and weston_seat . Choosing a backend before waking up compositor is necessary, otherwise you would not process any events. Now after After the waking of weston_compositor , before wl_display_run , you can have very few additional setups on the compositor. xkb_rules are already fixed, you cannot change the modes, or scale of enabled output. But you can still create some wl_globals for your needs. Finally, calling the wl_display_run would start processing the event queue. A quick run on the wlroots. You start again, by creating wl_display , wlroots does not have a compositor object, instead, you work with wlr_backend , it abstracts away the hardware events and rendering callbacks. As metioned before, wlroots heavily relies on signals, wlr_backend is a perfect example. By adding listeners to events like new_input and new_output . You can handle objects like wlr_output and wlr_input_device . Each of those objects has their own events, such as wlr_input_deivce.events is used for input handling. You additionally creating objects like wlr_seat (which is a protocol implementation) for forwarding input events to your clients. This is a critical point actually, if you decide to go with wlroots objects, from this point you will need to go all the way. If you decide to implement your own wayland interfaces, you can forget all the types like wlr_seat , wlr_surface , wlr_compositor , etc. Back on the topic, in creating the wlr_backend , you also would want to create your own renderer or not, wlroots comes with a very renderer. After all those objects created, press the button wlr_backend_start for launching the compositor, a bit similar to weston_compositor_wake right? If nothing goes wrong, you would see a window by now. There are many other details needed to filled, since wlroots is a modular library, every protocol you want to support needs to be added explicitly, you probably want a xdg_shell for example. In the end, call wl_display_run to start the event queue. Summary I hope this article would be useful for getting an idea about wayland compositor librarys. There are some design I preferred in libweston but in general I would prefer the modular design of wlroots , it looks much less intimidating. You can understand a type at a time, Where in libweston, every c file in libweston is like a few thousands lines long, understanding the project and adding modification is difficult, though it is a more mature project.","tags":"misc","url":"/libweston-vs-wlroots.html","loc":"/libweston-vs-wlroots.html"},{"title":"wl_data_device","text":"wl_data_device interface in the base wayland protocol is the most complex one, in my opinion, wayland.xml explains it fairly well, but every pieces are scattered in the file, here I serialize it together, give you quick run through. In brief, data device in wayland consists of four interfaces: The first is wl_data_device_manager , it is the global. The second is wl_data_device , can be created from wl_data_device_manager for a given seat, it is used to set selection(ctrl-c ctrl-v) and drag-n-drop. The 3rd is wl_data_source , also created from wl_data_device_manager , this source will be either a selection source or drag-n-drop source. The last is wl_data_offer , which is used for copying data from wl_data_source . Now we talk about the dynamics of the those 4 objects. Firstly a client creates a data_source (for example you pressed ctrl-C), gives it all the MIME types it offers and mark it as a selection or a drag. Then it is server's responsibility to create wl_data_offer at a proper moment later(either a new surface focused on a keyboard or pointer moves in a surface). Once the data_offer created, server will immediately send all the acceptable MIME types ot the offer, expects one of them accepted by the offer. If it is true, the data_offer need to give a file desriptor for writing data to, the server only need to transfer the call to the data_source and it writes to that fd . After it is done, data_offer is happy then send a wl_data_offer.finish to server can destroy the source . This process roughly covers the selection process. The drag-n-drop is a little more complicated. The different in drag-n-drop starts with wl_data_offer creation. As we said before, it happens at wl_surface.enter , here we would call wl_data_device.enter for the data_offer . wl_data_device.motion is called on cursor movement. Note that pratically you can drag through the entire surface without releasing, then on the next surface, server will call wl_data_device.leave first, then create another data_offer amid wl_data_device.enter . During this process, there is a side story happening, wl_data_source and wl_data_offer needs to negociating the actions(copy/move) and acceptable MIMEs. When cursor released, data_source is notified with wl_data_source.dnd_performed , finally it would be like in selection, data_offer gives a file destriptor and data_source to write and wl_data_offer.finish is used to finish the transaction. Overall it is more complex protocol, multiple requests and events are executed in sequence and expected in a given order. This is unlike most interface, where requests/events are more like a single shot.","tags":"misc","url":"/wl_data_device.html","loc":"/wl_data_device.html"},{"title":"May 2020 Status Update","text":"Forcing a monthly blog update seems to be a good idea, giving myself a reason to keep paying for this Domain. Also, I don't usually has a topic to address specifically, a monthly update can fill this role. Anyway, time passes rapidly in quarantine my friends. I hope a few years forward when I look back at this moment, I would not feel I wasted so much time. I don't actually feel I made much progress through this month. I would say, I am digging into my memory right now, it is hard to recall much in detail. yeah, that is life in quarantine, \"stagnation\", like the blackwater. Excepting that I announced taiwins on the wayland mailing list. It was, suprising I would say, more precisely, I didn't have any idea what would happen. I got a handful people stared my project. I guess there are quite a few people be interested in switching to wayland, but unfortunately there were no people showed up much interest in helping developing it(let me be wrong please), which I obviously need the most. Taiwins was my quiet little personal project for this long(and also my biggest social life barrier, on the other hand), my father said it would be waste of time if nobody would use it. Maybe taiwins is that window manager that only fits my niche? I only hope later when it is more mature, I will be proved wrong. At this point, If I have to make a judgement, I think taiwins would be more like a lightweight DE like Xfce and lxde than compact window manager like i3wm , I would say in between. Would that be a niche market? Only time will tell I guess. Right now taiwins is far from a daily drive. I have to push it forward, next feature would be primary-selection and real useful website. Another thing I need to think right now is switching to use wlroots, it would be a painful refactoring. But it looks like libweston is really not going to the way I hoped. Another thing worth mentioning is that I started to implement the animations system, wow this is the hell-I-remember many times I have to review on quaternions. And its interpolations. Interpolating multiple quaternion samples is hell of a job. How do I find the derivatives of quaternions anyway? Next step would be finishing the cubic and bezier interpolations on quaternions, before I can move to skinning. Well, until next time.","tags":"misc","url":"/may-2020-status-update.html","loc":"/may-2020-status-update.html"},{"title":"Busy in quarantine","text":"I update this blog less frequent than once per year, which sometimes makes me question why do I keep paying for this site. Be able to update blogs is always my pursuit, unfortunately there were always more code to write. It has been more than one year, last time I was still on how to MSAA on OpenGL 4, somehow it felt like a decade ago. The peaceful developing world took an interesting turn. A long run bull market is killed and NYSE triggered circuit break 4 times in 10 days. People are joking they now have 75% of Warren Buffet's experience. Will this gonna be the story we told our grand children born after 2020? \"You know back in 2020, it was a crazy year...\". We just saw the quarter of it, I am kind of expecting the crazier in the rest, will it be massive unemployment and a scary recession will be waiting? This week the market has recovered a bit of its confidence somehow, despite the Covid-19 cases are surging. A friend used to tell me, it would be a point where many will be killed by the virus and people just don't care anymore, looks like the market is at that point. It is the third week I am at home quarantine, but it has been a busy ride. I never got the chance to actually have this much free time to work on my own interest. Well, I merged 4 branches for taiwins so far. shockingly, it actually has this much work to do, if not this quarantine time, I probably will never have this much progress. It was supposed be finished in 2018 and I thought, \"well, I should be finished soon\", then 2019 passed, there I just finished a console feature for launching applications and I had the second time wishful thinking. Now in this year I had to add features like dbus , tw_theme , it looks like necessary. I did not think a dbus feature toke me a month to develop, nor did I think a tw_theme would cost 2 full weeks, not to mention I have not yet finishing patching weston in upstream. All those toke trumendous amount of time. At this point I feel lucky for the coronavirus outbreak in someway, I wouldn't be able to advance this much in progress. In am running out of time, in a few weeks I will be taking buses to work again. Then I would be back on track on my proprietary software job.","tags":"misc","url":"/busy-in-quarantine.html","loc":"/busy-in-quarantine.html"},{"title":"Realistic Deferred MSAA implementation","text":"Deferred MSAA, always has been a good problem. In the spatial anti-aliasing domain, MSAA is still the swiss-army knife, handle almost all the case. Some other post-processing methods like nvidia's FXAA, AMDs MLAA, or DLAA. FXAA is rather pleasing in many cases as well, especially if you are a video game developer, as long as your rasterization implementation does not screwed up. But for the case like grass rendering, fur rendering, when you have many layers of thin line, FXAA will fail you. Just like the pixels annotated in the image below. FXAA issues, it the rasterization failed to produce fragments, FXAA cannot help Other option like Nvidia's AGAA(aggregated G-buffer anti-aliasing) Or if you are really a game developer, TAA maybe the way to go. But here we are talking about good old MSAA technique(also, refer to this article if you can have non-even sample depth buffer). Here we are talking about MSAA, with only OpenGL 4.0 capability, no special vendor requirement. The trick is simple and effective: \"MSAA edge detection\". If we can detect difference between simple pixel and complex pixel, we can treat them differently. It is the similar idea from this method . In the D3D API, you can use SV_Coverage , in GLSL, you have similar variable GL_SampleMaskIn , these are the variable which are available in the pixel shader, tell the GPU which sample to write after shading. Take 4xMSAA for example, if all the samples come from the same fragment, then the sample mask is 1111 for that pixel(4 bits). If the fragment only writes to 2 samples, the sample mask would be 1010 , 1100 0011 , 0101 , etc. Which in turns mark this pixel complex. Complex pixels here are marked on the edges, whose sample mask is not full There is only one problem here. Through my experiement with opengl, if the pixel is not on the triangles edge, but on the intersection of two triangles, the sample mask does not reflect this problem. I guess that is why when you look the came at the ground in the video game, there is still aliasing effect at the intersection of grass and terrian. To solve this, you can apply apply a normal test on the samples, if the pixel locates on the intersection, the normal test fails and thus mark the pixel complex. Voilà, here is your deffered MSAA, it runs really fast for the lighting pass, and it works all the time, unless(there is always an exception), you have co-planar z-fighting problem, then you would have probably bigger problem to worry about than anti-aliasing.","tags":"misc","url":"/realistic-deferred-msaa-implementation.html","loc":"/realistic-deferred-msaa-implementation.html"},{"title":"The wayland project II","text":"The last post I laid out the story of me with wayland. Technology is fascinating isn't it? Every once a while, there are plenty of new projects that aim to start an revolution, getting people excited. Projects like systemd , Wayland , Vulkan make us think how come we did not think of those before, they seemed perfect at the moment. Technologies always work like a rush of hot wave, our sights are limited at the moment we are in, maybe 5 years from now, even vulkan is not sexy anymore. For ten years, community has been urging everyone to jump on the boat of wayland. Some major platform adopted it, just there is no money in it. Major platforms like GNOME and KDE are not even mature yet, KDE developers also worries about the future of the wayland, will it last 30 years like X window ? One of the truth that almost nobody talks about is that we all try to advertize the vague new technology as easy-to-learn, benifits and ignore all the learning curves. Is it really so? How many simple ideas out there that can rock the world? I cannot judge, in the context of wayland project. I cannot really use the word facile . The knowledge required for a wayland compositor is far more than what presented in DWM . Wayland is merely a IPC protocol library, all the work such as mapping hardware to logic devices, rendering and compositing has to done by a compositor writer. Even with the library libweston , programmers are still exposed by all those concepts. If you step on the path to do it all yourself, you will need quite a massive knowledge under the belt, I'd say. But let's not discourage everyone. Let's talk about all the goodies of wayland tech, especially the libweston could offer. The input The first thing that I started with the libweston is actually learning xkbcommon (xkbcommon itself has the concepts like keymap , modifier , keysym and so on). I was trying to implement a complex keyboard mapping, supporting sequence keybindings. It works a bit like what is available in emacs, only I have to hack around the libweston 's flat keyboard handler. After registering the keybinding callbacks, compositor will route the key events to the clients when they are in focus. Another idea exposed is the seat , it is presented in the wayland core protocol as well. Unix system has this concept called seat . A seat is a set of input device like keyboard, mice or controller, it means adding a seat for a user to user, how to discouver and group input devices into seats is transparent in the libweston, what programmer has is a signal handler. once a new seat discovered, compositor can setup the keymap and other things. The view and surface The view and the surface are the core concept in the weston composition. Basically weston_surface contains the buffer, but weston_view determines where you will show the surface on the screen. In the current weston architecture, at every frame, weston will build up a list of views and render them on top of each other. You can play all kinds of tricks with view, like rotations and scaling. weston_output and weston_layer The next concepts is the layer, this is idea when you want to futher seperate compositing into layers of cake. For example, on the top of your layers, you would have your cursor, then you may have some ui elements. futher downwards, you may have your regular windows section, then at the bottom, there is the wallpaper. If your GPU supports multiple planes, you can take the advantage of it to composite only certain layers and leaves the rest unchange, your wouldn't want to redraw everything just for the small motion of cursor, right :p? What about weston_output ? Output is the monitor, but technically it is a view of your entire framebuffer. If you have two monitors side-by-side, one output (monitor) occupies a chunk of that framebuffer. In this design, we only need to composite one framebuffer, and you can move the windows from one monitor to another at ease. Other wayland concepts Wayland object is not easy to work with, first challenge is double buffer state. Manage the state is quite lines of code actually. Many other objects like registry and wl_global or wl_seat are easier and usually done with a common routine (I guess that is why you would want Qt/GTK instead of barebone wayland client). But The most confusing concept I've had is the wl_surface_frame . By reading the wayland docs. Firstly I thought it was compulsory to do a buffer swap, once you have your frame callback, you can do a rendering and commit. But if you really do that, you would not be able to draw anything since frame only issues after the commit . So you need to do a wl_surface_frame , then commit, then why do you need the wl_callback later? Just to delete it? This was how I drive my wayland client for a long period. In turned out wl_frame is only useful for the animations, but even though, you still need to have a first commit to drive the next frame. If you are work with typical GUI element where you only need to redraw at input events. The frame callback is totally unecessary. I only realized it after reading through the libweston rendering pipeline. So if you need to work on a wayland client, there are some traps like this. Designing UIs under wayland is really far from easy, I would maybe write another article about that part of the story. So after all, is it all good In the end, I have to say this. With help of libweston, the implemention of a compositor is indeed much easier, it setup the drm and gbm , and implements most of the core protocols like wl_surface wl_buffer on the sever side. What it offers the compositor designer is a feature-rich apis to manipulate wayland object, you can do almost everything you want. Only if you have a strong grasp of all the concepts, so you can avoid all the traps that I as in. And libweston has absolutely no documentation at all.","tags":"Journal","url":"/the-wayland-project-ii.html","loc":"/the-wayland-project-ii.html"},{"title":"The wayland project","text":"It was a good will. 5 years ago, I read a blog about the future of linux desktop, I was using a window manager called Awesome (one of my friend introduced it to me, I was enchanted by how he opened two terminals side-by-side, one for coding and one for compiling). A voice about next generation desktop was just raised in the linux community , a group of hackers demod the weston project, where you can rotate the application windows. They also pointed out the existing problems in the x server architecture(basically it is too old, let us just replace it). It is cumbersome, too many components, easily dead after package upgrade. Wayland vs x server architecture X did not die, it lives well even now. It did not stop me from starting a wayland window manager project though, it was only 3000 lines in the DWM (I still use it in my laptop) project, how hard can it be? Then I looked into the wayland protocol and the only wayland client programming tutorial available online(except those examples from weston itself). It did not seemed that hard, altough I did not understand very much about how wayland double buffer works, what does the wl_callback . The client side programming seemed a bit complicated than SDL or Qt , but it was doable to write simple applications. But how do we create a wayland server? Could it be just 3000 lines as well(yeah actually it may be if I am really smart, it is 6000 lines now)? The first problem I had was mode-setting , I did waste enormous time writing the drm code . It seemed will take forever. I did not know what it meant for a wayland server and what I need to code for it. Spolier alert, It really takes very much for wayland compositor than a X window manager. Anyway, by the time I was familiared with libdrm code, some one started trying to write the wayland compositor library , (I am not sure the exact time, maybe there was a gap while I did nothing because of the lacking of compositor libraries) there was one that you can open a wayland server then have a small cursor on it, and that was it. Then there was swc , the video looks really nice, very responsive. It was designed to create simple compositors. It can work as well on Nvidia GPUs (nvidia only has wayland support from last year I think, they do not give a shit about libdrm, when they finally supported libdrm, they came with a different solution). it is now completed deprecated, I think the developer stopped development even before I knew how to work with it. It was easy to start a wayland compositor, but afterwards, it did not have any documents about how to manage the windows. Then after a few monthes, a appealing project called wlc started gaining attentions on github. I still rememeber the architecture, the first design was creating a structure and let user writes the callback to inject into it, they even have default ones if you do not want to implement the specific part. My first trial on wayland compositor was based on that. Another famous i3 compatible weston compositor called sway emerged from it. It worked but inadiquately, it seemed to be always laggy and consumed too much resources that I did not know how, sway still has these performance and resources consuming problems. Yes, I forgot to mention my project name. it is called taiwins , while I was satisfied working on the wayland server without the need to start from libdrm . I somehow made it a C++ project when I was note competent with it, without STL , because I thought template brings too large binaries and compositor has to be as light as possible. It was dumb idea, it could save a lot of time if I did not need to write containers like vector and hash tables. Anyway, without template and stand libraries, cplusplus are truly horrible, I cannot take bit of advantages from the power of cplusplus. Maybe just helping me learning cplusplus. That was the story ends in 2017. Then when weston-3 released, the game changed a bit, for the first time, weston developers realized that it may be a better idea to make weston a library rather than just showing off what wayland can do. KDE starts to port their KWin to wayland, Gnome started a long time ago with mutter but finally usable, Fedora 26, I think, was the first linux distribution that came with wayland support. At the same time, wlc library stoped the development as well, the author mentioned that he had no time maintaining it so sway moved to its wlroot library. At that time, I switched to use libweston for the performance and likelihood of maintainance. Again, from the scratch this time, purely in C. Since I dislike the idea using a crippled cplusplus in a project.","tags":"Journal","url":"/the-wayland-project.html","loc":"/the-wayland-project.html"},{"title":"Rotations","text":"Rotation, combined with translation and scaling, are the three affin transforms we do every day in the 3D nutshell universe. The rotation itself, however, is somehow much more complicated than the other two transform, it is one really needs a matrix representation among all three. Representations and computation of it has been developed for years. We have systems like Axis-angle representation, matrix representation , euler angles and quaternions . Despite I have known them for a long time, when I forgot, the rotation is still complicated. Here I am writing this again, as easily understanding as possible, for my future-self (or I can just travel through time to ask myself now). in 3D space, we think a rotation is about an axis(in 2D you can also do it, but the axis is out of the space, and yeah, higher dimension is out of my reach, just forget about those). This leads us to the rotation vector representation \\((W, \\theta)\\) where \\(W\\) is the rotating axis, and \\(\\theta\\) is the rotating angle. This representation is just for sake of looking. It raise the question like how do you rotate a vector or point with this \\(\\theta\\) and the axis algebraly? Well, follow the Rodrigues formula \\(v_{rot} = \\cos\\theta v + \\sin\\theta (w \\times v) + (1-\\cos\\theta) (w \\cdot v) w\\) . Or ease the pain for the head, you can re-represent in Matrix then do the dot product. Well, but, this matrix form, is confusing when you look at it if it is ever more complex than a 2D transform, so you could dicompose the rotation into 3 2D rotation matrices. Anyway, I feel that we cannot avoid the rotation table here, they are Bruce Banna in different forms of hulk(clear throat). Name representation N operations on vector combination interpolation rotation vector \\((w,\\theta)\\) 4 Rodrigues' rotation impossible hard Matrix \\(M\\) 9 \\(M \\cdot v\\) \\(M_1 \\cdot M_2 \\cdot v\\) hard euler angles \\((\\phi,\\theta,\\psi)\\) 3 \\(\\psi(\\theta(\\phi(v)))\\) cascade hard quaternions \\(q=(w,x,y,z)\\) 4 \\(q\\cdot v \\cdot q&#94;{-1}\\) \\(q_1 q v q&#94;{-1} q_1&#94;{-1}\\) slep Let's play the role of doctor, dive into the head of rotation and see which causes him to turn into different \"hulks\". The rotation matrix is the most common \"hulk\" we see, green, big and it smashes everyone with its multiplication. But what it really does with its gamma (okay, enough) what the matrix really does is projections, or in other words, changing basis. It is better to see with the example of a extrinsic matrix, if I rotate my camera -45 degrees based around y axis, what we see in the camera is that my object rotate 45 degree around y axis. Reversely, if I want to rotate the object 45 degree around y axis, I can rotate my camera -45 degree around y axis, same for the x and z axis. Then express the change sous la forme de change of basis. Then we re-project this new world with this new camera. Everything worked out fine. $$ M = \\begin{bmatrix} R_x & R_y & R_z & 0 \\\\ U_x & U_y & U_z & 0 \\\\ D_x & D_y & D_z & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} $$ rotation matrix rotation vector euler angle quaternions rotation matrix x decompose solving equations solving equation rotation vector \\(RR_{2D}R&#94;{-1}\\) x \\((w,1/2\\theta)\\) euler angle \\(R_p R_y R_r\\) x x quaternions Matrix formula \\((w, 1/2\\theta)\\) x x So what is the relationship between rotation matrix and rotation vector? Remember that an axis-angle rotation is around an angle? So let us align our normal vector of the image plane of the camera to the rotating axis, this is the same as changing basis, let us call it the basis \\(R\\) . Then we use a simple matrix for expressing an 2D rotation \\(R_{2D}\\) . Finally after we are done, we need to rotate the camera back using a inverse matrix \\(R&#94;{-1}\\) , Parpait! As much as we love the rotation matrices, it is too heavy for many people. We can paint our world of rotations with only 3 numbers (yaw, pitch, roll). By the name of Euler, the angle is also called euler angle. Euler angle is the The Green Scar whom everyone likes, he waves a hammer and a shield, defeated everyone in the arena, years later, he became the story of legend. Words about him spread among young children's ears. Okay, pratically, euler-angle is cascading the transform on different axises. So if we rotate two of the axis so that the third axis became the rotating vector in the angle-rotation representation then we see our old friend in a different suit. Transforming to matrix is as easy as 3-product of 2D transform \\(R_p R_y R_r\\) . It is harder to come back from the other end, but you can still do it by solving the matrix $$ R(\\phi) R(\\theta) R(\\psi) = \\\\ \\begin{bmatrix} \\cos \\theta \\cos \\phi & \\sin \\psi \\sin \\theta \\cos \\phi - \\cos\\psi \\sin\\phi & \\cos\\psi \\sin\\theta \\cos\\phi + \\sin\\psi \\sin\\phi \\\\ \\cos\\theta \\cos\\phi & \\sin\\psi\\sin\\theta\\sin\\phi & \\cos\\psi\\sin\\theta\\sin\\phi - \\sin\\psi\\cos\\phi \\\\ -sin\\theta & sin\\psi \\cos\\theta & \\cos\\psi\\cos\\theta \\end{bmatrix} $$ The last bit of the story of euler angle here, we have to mention the gimbal lock, which is so elegantly designed to cause a dimension drop, a singularity that traps the earth to a 2D plane, no escape, thus brings the end of world (clear the throat). Okay, the gimbal lock lies in the second axis, when you turn 90 degree, you will align the first and the third axis together. Then rotating on first and second axis is to the same axis. Okay, I finished. The last one of the family, is the Quaternion , he is the \"perfect\" hulk, with a face and intelligence of Bruce Banner and the strength of the hulk. The only problem is the difficulty to understand. The math of quaternion is skipped here(I am too lazy, maybe one day I will come back to fill it up). We can note done in the mind that \\(i&#94;2=j&#94;2=k&#94;2=ijk\\) and \\(q=(\\cos\\frac{1}{2\\theta}, n_x\\frac{1}{2\\theta}, n_y\\frac{1}{2\\theta}, n_z\\frac{1}{2\\theta})\\) without understand it. It is \\(\\frac{1}{2\\theta}\\) not \\(\\theta\\) , why!!! Well, it because \\(i&#94;2=j&#94;2=k&#94;2=ijk=-1\\) . So you need to rotate 720 degree to get back to original state, sounds like there are some higher dimensions beneath. In the end, We can also successfully transfer a quaternion into rotation matrix without going though the rotation vector, which makes our lives easier.","tags":"Computer Vision","url":"/rotations.html","loc":"/rotations.html"},{"title":"Bone Animation [part I]","text":"I have been trying to create a animation system for my OpenGL Project for a long time, one of the reason is I have limited amount of time after starting the full-time job. Another problem was that, I mean, if I intend to keep it a clean project rather than just a school project, building a animation system is like a rabbit hole, 30 lines of code got me another 100 lines of work, it only leaded me deeper and deeper. Well, it is a perfect opportunity for me to explain the story here. The amount of skeletal animation that I found online, especially good blogs are less than a dozen. Gladly, I would like to point it out here, there is a good youtube video series you can follow, it is in Java, the author provided the source code for reference. Khronos has a shader example here and there are a few others. Well, before we can draw anything in a graphic program, we need to thank artists for the rigging part. Connecting the dots between bones and meshes in the blender is far from a easy task, I gotta say that from my personal experience, I gave up in the first step, leaving my ambition of replacing all the artists my programs in the code water, so machine learning programs will take their years to graduate from art school. While they doing that, we still need to make friends with artists and draw our mesh with opengl. Alright, back to topic. What do we need, for skeletal animation, exactly? Let's take a look of the diagram first. I hope this diagram is not missleading too much. Structure , transformations and skinning are the three parts we need to take care of. Well, from the history books you know that every charactor has a set of bones structured in a tree, and a bone has transformations which affects itself and its children which also affects the assosiate meshes and... Before we can start draw anything, hundreds lines of code just for logic need to be done. It is really against the practice. Alor, afin de dessiner en OpenGL, we need to feed the shader program the minimum amount of data it needs, apart from normal, vertex and texture, two extra layouts of bone and weight we need to give it to the shader. If you don't want to do any transform, we don't exactly need the real bone weights and transforms. On the CPU part, the work lies on assosiating the bones and meshes . Depend on the asset library using, the data is structured in different ways. For instance, assimp requires user to read bone lists from a mesh, where you can read the bone weights. and also bones hierachy is stored as aiNode , where you read the cascaded transforming matrices. On the GPU part, our vertex shader program looks like this: //[Vertex Shader] #version 330 core layout ( location = 0 ) in vec3 position ; layout ( location = 1 ) in vec3 normal ; layout ( location = 2 ) in vec2 texCoords ; //we can also make a matrix4x2 layout ( location = 3 ) in vec2 bw0 ; layout ( location = 4 ) in vec2 bw1 ; layout ( location = 5 ) in vec2 bw2 ; layout ( location = 6 ) in vec2 bw3 ; out vec2 TexCoords ; out vec3 fragPos ; out vec3 Normal ; const int maxNbone = 100 ; //it has to be constant uniform mat4 MVP ; uniform mat4 model ; uniform mat4 boneMats [ maxNbone ]; void main () { vec4 v = vec4 ( position , 1.0 ); vec4 n = vec4 ( normal , 1.0 ); vec4 newVertex ; vec4 newNormal ; //updating vertex newVertex = ( v * boneMats [ int ( bw0 . x )]) * bw0 . y + ( v * boneMats [ int ( bw1 . x )]) * bw1 . y + ( v * boneMats [ int ( bw2 . x )]) * bw2 . y + ( v * boneMats [ int ( bw3 . x )]) * bw3 . y ; //updating normal newNormal = ( n * boneMats [ int ( bw0 . x )]) * bw0 . y + ( n * boneMats [ int ( bw1 . x )]) * bw1 . y + ( n * boneMats [ int ( bw2 . x )]) * bw2 . y + ( n * boneMats [ int ( bw3 . x )]) * bw3 . y ; gl_Position = MVP * newVertex ; Normal = newNormal ; fragPos = vec3 ( model * newVertex ); TexCoords = texCoords ; } Straightforward as it is, since we don't have the cascaded transform here. We just set all the boneMats into identity matrix. It looks as same as a rigid object shader program, we will come back with bone transform next time.","tags":"misc","url":"/bone-animation-part-i.html","loc":"/bone-animation-part-i.html"},{"title":"Template Deduction for C++","text":"Template is a confusing and hard features in C++ if you want use it well, and I think it is also one of the reason which causes c++ programmers cannot understand each other. Sometimes the Deduction rule for c++ is really confusing, you don't understand why you code doesn't work or why it works. And Since STL heavily uses nesting Templates, both your compiler and your mind suffers from that. Function Templates People commonly start define and using templates for Container, such as std::vector and std::map . Soon later, they want to write template functions instead of overloading. The sort function from standard library is a good example. It is usually defined as: template<class RandomIt> void sort(RandomIt& first, RandomIt& last). Then you can sort a vector by: std::vector<float> a = ...; std::sort(a.begin(), a.end()) The compilers try to infer the template parameters from template functions' parameters. Sometimes compilers are not smart enough to deduce it out, then you have to specific it yourself. The deducing mechanism is a choice of compilers. For example, compilers never tries to deduce the type for a class template. Compilers deduction policy is hard to implement, and quite often, I blame for compiler's stupidity for not be able to deducing the templates(but of course, the guys that wrote compiler usually smart than us). But for us, we need to know when we don't need to specific the template parameters, when we will need to. According to CPP Reference , the difference is there are some context participate into deduction, others does not. Template deduction for template functions can be done with function arguments. Compiler never deduce templates for class. Template deduction cannot be done with return types. Nested types such as std::vector<std::complex<double> > can causes troubles. A template type that uses another template, like F& (*function)(T l, T r) , here F is not deductible. And there are many other rules. Looks like now, the deduction can only be done with direct type that list in code.","tags":"Programming","url":"/template-deduction-for-c.html","loc":"/template-deduction-for-c.html"},{"title":"Alternative usage to `typedef` in C","text":"Variable length array(in runtime) and Typedef array type GNU C's variable array declaration is very convenient, instead of int *a; then use malloc(sizeof(int) * size) then copy data with memcpy (3 steps required), you can directly code with int a[size]; . There are another feature you may want to know, you can define a type at any size you want by typedef char data_t[size] , this feature is really good because it can be done in runtime, this provide a short cut for indexing void * . Instead using (char *)init_ptr + ind * elem_size' for indexing, we can replace it by (data_t *)init_ptr[ind] . This could save many indexing problems, but the bad news is that we need to define the type by the storage many times.","tags":"Programming","url":"/alternative-usage-to-typedef-in-c.html","loc":"/alternative-usage-to-typedef-in-c.html"},{"title":"View Matrix for projection","text":"The MVP Matrix \\(\\text{Model} * \\text{View} * \\text{Projection}\\) is the first lesson to render (the so called render is the process so that people can see it on a 2D screen )objects in Computer Graphics, which transfer a 3D object in object space into, in the end, a UV plane. The Model matrix is simple and easy to understand, simply the translation, scale and rotation, but View matrix and Camera matrix are not obvious(although you can get it for free by single call from glm::lookAt() and glm::perspective() ). How does View Matrix work? The view matrix has another name called extrinsic matrix in Computer Vision, people use it to find the where the camera is. The engines don't move the ship at all. The ship stays where it is and the engines move the universe around it. This simply means that the view matrix does nothing but remapping everything from $(0,0,0) to the centre of the camera. By linear algebra, it is a linear transform that changes the basis. and one can use the glm::lookAt() generates the view matrix. So in the beginning, the camera sits at \\((0,0,0)\\) , and looking at \\((0,0,0)\\) . The normal is \\((0,1,0)\\) , since we don't know the direction, lets assume it is \\((0,0,-1)\\) . And imaging the universe is a huge Cube box that surround us. If we want to move the camera to left by \\((-3, 0, 0)\\) , we can translate the cube by \\((3,0,0)\\) If we want to rotate the camera to left by 30 degree, we can rotate the cube by 30 degree to the right. So the inefficient implementation is simply just -translation * -rotation , But about the rotation part, there are simple way to do it. Called Gram-Schmidt process. The essence is, again, projection, if we want to retrieve the coordinates from one xyz coordinate system to our new coordinate system, we can simply projecting to that system by dot product to the new axies. The complete View matrix format is: $$ M = \\begin{bmatrix} R_x & R_y & R_z & 0 \\\\ U_x & U_y & U_z & 0 \\\\ D_x & D_y & D_z & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 & -T_x \\\\ 0 & 1 & 0 & -T_y \\\\ 0 & 0 & 1 & -T_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} $$ \\(R\\) , \\(U\\) , \\(D\\) is the new coordinate basis, the principle is super simple, simply by first reverse-translate the point and second projecting on the new coordinate system. In shorter form: \\(M = R | t\\) . The persepective projection Persepective projection, on the other hand, is a way to project 3d sences to 2d plane, as the way of human eyes and camera. Which means the object further from us looks smaller than the object closer to us. It sounds nature, but how does the computer implement it? Thats where Camera matrix were introduced. Camera matrix To finish projecting objects to our eyes, we need to follow the formula that make futher objects smaller. Given two points ([x_1, y_1, z_1] ) and ( [x_2, y_2, z_2 ]), they would project to the same position if ( x_1 / z_1 = x_2 / z_2 ) and ( y_1 / z_1 = y_2 / z_2 ). The projection is to project ([x, y, z] ) to ( [d\\frac{x}{z}, d{y}{z}] ), the ( d ) there is the camera plane. Since there is now linear tranform to do that with 3d matrix, we have to use homogeneous coordinate. $$ \\begin{bmatrix} 1 & 0 & 0 & 0\\\\ 0 & 1 & 0 & 0\\\\ 0 & 0 & 1 & 0\\\\ 0 & 0 & -1/d & 0 \\end{bmatrix} \\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\ \\end{bmatrix}= \\begin{bmatrix} x\\\\ y\\\\ z\\\\ -z/d\\\\ \\end{bmatrix} $$ And as homogeneous coordinates, we should keep scale to keep last element to 1. $$ \\begin{bmatrix} x\\\\ y\\\\ z\\\\ -z/d \\end{bmatrix} \\rightarrow \\begin{bmatrix} -d\\frac{x}{z}\\\\ -d\\frac{y}{z}\\\\ -d\\frac{-d}\\\\ 1 \\end{bmatrix} \\rightarrow \\begin{bmatrix} -d\\frac{x}{z}\\\\ -d\\frac{y}{z}\\\\ \\end{bmatrix} $$ We can simply replace ( 1 ) with ( -d ) in the projection matrix to reach the same goal. $$ \\begin{bmatrix} -d & 0 & 0 & 0\\\\ 0 & -d & 0 & 0\\\\ 0 & 0 & -d & 0\\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} x\\\\ y\\\\ z\\\\ 1\\\\ \\end{bmatrix} \\rightarrow \\begin{bmatrix} -dx\\\\ -dy\\\\ -dz\\\\ z\\\\ \\end{bmatrix} \\rightarrow \\begin{bmatrix} -d\\frac{x}{z}\\\\ -d\\frac{y}{z}\\\\ \\end{bmatrix} $$ Finally, the camera matrix looks like this: $$ \\begin{bmatrix} -fs_x & 0 & x_c\\\\ 0 & -fs_y & y_c\\\\ 0 & 0 & 1 \\end{bmatrix} $$ Its little bit complex than what we have, but general idea stays the same.","tags":"Computer Vision","url":"/view-matrix-for-projection.html","loc":"/view-matrix-for-projection.html"},{"title":"The CMake ExternalProject module","text":"When you need a external project Everyone knows not to write everything your own. But there are those you can install by apt-get , those you cannot. messing up with your /usr/local/ directory is not a long term solution, especially when you just need to try a piece of shitty code. So here is the problem, I just need to compile the dependent project and link to them without modify their Makefile . CMake's ExternalProject module is what you looking for. ExternalProject Module","tags":"Programming","url":"/the-cmake-externalproject-module.html","loc":"/the-cmake-externalproject-module.html"},{"title":"Everything you do I can do it with a double pointer in C","text":"It is 2016, low level coding is not needed anymore. We have programming languages like javascript and python. When people code everything with a class and related methods. But do you remember we have a programming language called C? When we had no class, templates, operator-override or implicit constructor at esthat time. You actually know what are doing with C, no magic! I claim no expert to programming, since I only have a four-year-old programming life. But even you are inexperienced like me, you should still be able to code some thing in C. IT IS fatigue to code in C, you have no standard containers to rely on, you spend 10 hours to build a perfect deque data type which works on amortized O(1) complexity and has no memory leaks before you are able to work on your project. Awesome!!! But why? Why try to build a huge robust data structure in a non-OOP language? Instead of trying simulate constructors and destructors, you could always use a double pointer to do the job. Have you ever seen Linus Torvalds's double pointer code? It removes on special case where you would make mistakes. But really? what can you do with a double pointer in C? Well, Suppose I have vector data structure and I didn't want to write the code like this: struct vector { size_t elem_size; void *arr; } we can actually just use a simple array to do the job. If you just pass the void * to insert function, how can I modify the size of the vector, or insert anything before header? This should not be a problem with this: void insert ( void ** arr , void * elem , size_t * remain_size , size_t * size ) { size_t old_size = * size ; if ( * remain_size == 0 ) { * arr = realloc ( * arr , 2 * * size ); * remain_size = * size ; * size = 2 ; } * remain_size -= 1 ; // copy the elem to the end }","tags":"Diaries","url":"/everything-you-do-i-can-do-it-with-a-double-pointer-in-c.html","loc":"/everything-you-do-i-can-do-it-with-a-double-pointer-in-c.html"},{"title":"6 years of programming","text":"I have been studying in programming for 6 years, since my first day of undergrad life. I haven't realized it has been so long, this studying life already took away most of my youth. I was a kid without knowing the toughness of life, to a young man need to worry about jobs and how to start having family. The computer industry changed vastly in this period. There was no Cloud Computing, Machine Learning was only known in Academics. Universities taught students how to code in VC++. Now everything is different. What was my path? Well, 5 years ago, a friend of mine taught me how to install Linux on a laptop, it all started there. Firstly it was Debian and Ubuntu, Debian was at version 5 and it did not even support WPA2 encryption. \"Why my laptop cannot connect to WI-FI?\", \"My X.org crashed again after I installed the proprietary graphics driver\", \"I ran that script, I don't know what is going on in the manual, it should work, right?\". \"How to get my speaker to work, I installed Alsa already\". I switched between Debian and Ubuntu for several times before I followed the installation guide of Arch Linux :p. I started to live under a terminal with 'grep', 'cd', 'ls', 'find', 'cat', etc. Then there is the story of how-to-vim. Adapting to hjkl, Esc and i, then you tried to copy other people's vim script, how to use ctags with vim, how to use YouCompleteMe. Writing dozen vim code without knowing why. VimScript is a hell of hack anyway. So my system was basically terminals with editors and a browser. Wait, I haven't talked anything about programming yet, should I started at \"hello world\"? No, lets started at CSAPP (if you haven't read it yet, you really need to, it is a good book), you know there is a stack and a heap in your memory, and modern systems use \"flat addressing\", between these two, people found there are still some space for shared libraries. Every instruction that CPU executes follow the same pipeline, and I can actually execute multiple instructions simultaneously. Do you want your program to hit the cache every time? Just joking. But you won't be able to find a job with C, right? Why the hell would any company would want employees to code in C? 2010s, it is all about javascript, this has been true for almost 6 years, I still haven't start learn javascript. But at least, I learned Python. Really, first time I did something besides computing and digits. 2012, it was just the time of machine learning, python was the first few languages that developed libraries with it. Numpy and Scipy, strangely, they were the first two libs I used just after the night I learned python(You can learn python in one night, it is true). That was just 2 years ago. Before I even realized my undergraduate life ended, the graduated offer came to my hand. I don't know what's ahead of me. But anyway, I was young. With a full head of unknowns, I flew to Canada. No more undergrad type of large classes, what you have in graduate life is just small classes and deadlines. Gee, they really passed so fast. 2016, I am in a Computer Vision lab now, just recalled the path I walked, just realized that my youth almost ends.","tags":"Journal","url":"/6-years-of-programming.html","loc":"/6-years-of-programming.html"},{"title":"如何写一个博客生成器","text":"这是我的第一篇github上的博客，感觉落后时代好多年啊。因为完成自己的博客生成器不久 。所以写的第一篇博客就以生成我的博客站点的项目 mkblogs 说起，来说说如何写一个博 客生成器吧。 现在利用github架静态博客可真是烂大街，但是毕竟我也就能做做烂大街的东西，所以。。 。额，首先还是简短的介绍一下 myblogs 脱胎自 mkdocs 项目，当时写 mkblogs 的原因是像看 看怎么写博客生成器。于是从 staticgen 找了一个比较简单 的python项目出来，打算用一下顺便学习学习，结果玩了一会发现它不是一个博客生成器。 纠结了一下感觉自己改一改吧，反正 mkdocs 才800多行代码。于是开始将一个文档站点生 成器改成一个博客生成器。随着我改代码的过程，看看写一个博客生成器需要怎么做。 一个博客生成器或者站点生成器的工作都是将人写的一个个markdown文件整合成一个有组织 的列表，这样就牵扯到两件事： 如何将单个markdown文件编译成html文件。当然这点不需要我们去做，好比说python的 mardkown模块已经帮我们完成了这个工作。 +如何将许多编译好的markdown文件组织成链表。这点需要我们去做。 处理Markdown到Html的转换 像Python-Markdown这样的库不会将源文件编译一个完整的html文件，就像这样： # 示例标题 示例文本 > 示例引用 1 . 示例列表 2 . 示例列表 3 . 示例列表 这里有一个 [ 链接 ]( anothermd . md ) 会转化成 <h1> 示例标题 </h1> <p> 示例文本 </p> <blockquote> <p> 示例引用 </p> </blockquote> <ol> <li> 示例列表 </li> <li> 示例列表 </li> <li> 示例列表 </li> </ol> <p> 这里有一个 <a href= \"anothermd.md\" > 链接 </a></p> 这里没有 <html> tag，也没有 <body> tag，这些可以借助Python-Jinja这样的库来完成， 我们需要处理的问题是重定位markdown当中的链接， anothermd.md 需要被重定位到 anothermd.html ，这样的工作需要我们写Python-Markdown的插件来完成。 Python-Markdown的插件系统十分灵活，可以在编译前，编译时，编译后三个时间来添加 Markdown插件，分别需要开发者继承 Preprocessor , Treeprocessor 和 Postprocessor 类，并重写 run 方法。具体可以参见 Markdown-Extension-API 。 当然你可以设计的更健壮一些，比如检查链接是否存在的问题。 构建HTML串来生成我们的完整站点 处理写完的MD按照分组构建出一串html，把他们联系起来就是博客生成器的主要工作啦。所 以实际上，你都不需要懂html, javascript, css网页三大工具╮(￣▽￣)╭ 。唉，又扯远了。 如何让用你生成器的人写博客完全看你心情，Jekyll貌似需要用户把写博客的时间加到 Markdown的文件上，然后它会按照时间顺序来组织你的博客。你也可以允许用户添加子目录 ，把关于一个主题的文章添加到一个目录中去，用隐式的方法来组织用户的博客。 或者你 可以支持Tag。 Mkblogs要求用户根据主题用的方式来组织博客，所以我们就说说这个方式是怎么做。其实 这又什么好说的呢，遍历一下文件树，逐个博客进行编译，最后生成一下主页，大功告成， oh year。是不是听起来很简单？但是说到细节，繁杂的事情就有一大堆了。但是话说回来 ，什么编程项目不是这样呢？ 为了不要烂尾，我在多说两句吧。为了能生成一个博客的站点，你总是需要在遍历文件树的 时候保留一些信息： 如果你的主页是记录最新的一些博客，那么就需要维护一个最新博客的链表。 如果你需要做归档目录，就需要维护每个主题目录的链表。 如果你需要添加一下aboutme这样的东西的话，就需要单独构建一些html页面喽。 我写的像屎一样的博客就这样生成了╭。","tags":"Journal","url":"/ru-he-xie-yi-ge-bo-ke-sheng-cheng-qi.html","loc":"/ru-he-xie-yi-ge-bo-ke-sheng-cheng-qi.html"}]};